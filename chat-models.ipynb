{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tool Calling","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade langchain-cohere langchain langchain_community transformers\n","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:01.633296Z","iopub.execute_input":"2024-09-02T18:14:01.633724Z","iopub.status.idle":"2024-09-02T18:14:08.395612Z","shell.execute_reply.started":"2024-09-02T18:14:01.633679Z","shell.execute_reply":"2024-09-02T18:14:08.394266Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdin","text":" ········································\n"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n* 'smart_union' has been removed\n  warnings.warn(message, UserWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"# The function name, type hints, and docstring are all part of the tool\n# schema that's passed to the model. Defining good, descriptive schemas\n# is an extension of prompt engineering and is an important part of\n# getting models to perform well.\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a + b\n\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    print(\"Multiply called\")\n    return a * b\ntools = [add,multiply]","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:08.397208Z","iopub.execute_input":"2024-09-02T18:14:08.397947Z","iopub.status.idle":"2024-09-02T18:14:08.404729Z","shell.execute_reply.started":"2024-09-02T18:14:08.397895Z","shell.execute_reply":"2024-09-02T18:14:08.403466Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"llm_with_tools = llm.bind_tools(tools)\n\nquery = \"What is 3 * 12?\"\n\nllm_with_tools.invoke(query)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:08.407890Z","iopub.execute_input":"2024-09-02T18:14:08.408420Z","iopub.status.idle":"2024-09-02T18:14:10.032028Z","shell.execute_reply.started":"2024-09-02T18:14:08.408359Z","shell.execute_reply":"2024-09-02T18:14:10.030644Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"AIMessage(content='I will use the multiply tool to calculate the answer to this question.', additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '4c82b846-9a50-438d-8db8-c35d41fc12e8', 'tool_calls': [{'id': 'fb8c3a07c8fd44cab16ff28df08527da', 'function': {'name': 'multiply', 'arguments': '{\"a\": 3, \"b\": 12}'}, 'type': 'function'}], 'token_count': {'input_tokens': 952.0, 'output_tokens': 61.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '4c82b846-9a50-438d-8db8-c35d41fc12e8', 'tool_calls': [{'id': 'fb8c3a07c8fd44cab16ff28df08527da', 'function': {'name': 'multiply', 'arguments': '{\"a\": 3, \"b\": 12}'}, 'type': 'function'}], 'token_count': {'input_tokens': 952.0, 'output_tokens': 61.0}}, id='run-e37cacb1-2f13-4956-a7ce-d3b136943d8d-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': '2f9f292f4f3044c88a17e6ea5e132172', 'type': 'tool_call'}], usage_metadata={'input_tokens': 952, 'output_tokens': 61, 'total_tokens': 1013})"},"metadata":{}}]},{"cell_type":"code","source":"query = \"What is 3 * 12? Also, what is 11 + 49?\"\n\nllm_with_tools.invoke(query).tool_calls","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:10.033408Z","iopub.execute_input":"2024-09-02T18:14:10.033779Z","iopub.status.idle":"2024-09-02T18:14:12.198150Z","shell.execute_reply.started":"2024-09-02T18:14:10.033740Z","shell.execute_reply":"2024-09-02T18:14:12.196914Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[{'name': 'multiply',\n  'args': {'a': 3, 'b': 12},\n  'id': 'f8af879317f9437b8bb5dccd07848ad0',\n  'type': 'tool_call'},\n {'name': 'add',\n  'args': {'a': 11, 'b': 49},\n  'id': 'cd40c25fe48b4429a3bed8953d469c97',\n  'type': 'tool_call'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Structuring chat model output","metadata":{}},{"cell_type":"markdown","source":"**With pydantic model**","metadata":{}},{"cell_type":"code","source":"from typing import Optional\n\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\n\n# Pydantic\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n    punchline: str = Field(description=\"The punchline to the joke\")\n    setup: str = Field(description=\"The setup of the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:12.199514Z","iopub.execute_input":"2024-09-02T18:14:12.199885Z","iopub.status.idle":"2024-09-02T18:14:17.295728Z","shell.execute_reply.started":"2024-09-02T18:14:12.199848Z","shell.execute_reply":"2024-09-02T18:14:17.294603Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Joke(punchline='Because she wanted to be a first-aid kit!', setup='Why did the cat join the Red Cross?', rating=7)"},"metadata":{}}]},{"cell_type":"markdown","source":"**With json**","metadata":{}},{"cell_type":"code","source":"json_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\n            \"type\": \"string\",\n            \"description\": \"The setup of the joke\",\n        },\n        \"punchline\": {\n            \"type\": \"string\",\n            \"description\": \"The punchline to the joke\",\n        },\n        \"rating\": {\n            \"type\": \"integer\",\n            \"description\": \"How funny the joke is, from 1 to 10\",\n            \"default\": None,\n        },\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\nstructured_llm = llm.with_structured_output(json_schema)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:17.298388Z","iopub.execute_input":"2024-09-02T18:14:17.298945Z","iopub.status.idle":"2024-09-02T18:14:18.569494Z","shell.execute_reply.started":"2024-09-02T18:14:17.298878Z","shell.execute_reply":"2024-09-02T18:14:18.568268Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'topic': 'cats'}"},"metadata":{}}]},{"cell_type":"markdown","source":"**Structured with streaming**","metadata":{}},{"cell_type":"code","source":"# from typing_extensions import Annotated, TypedDict\n\n\n# # TypedDict\n# class Joke(TypedDict):\n#     \"\"\"Joke to tell user.\"\"\"\n\n#     setup: Annotated[str, ..., \"The setup of the joke\"]\n#     punchline: Annotated[str, ..., \"The punchline of the joke\"]\n#     rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\n\n# structured_llm = llm.with_structured_output(Joke)\n\nfor chunk in structured_llm.stream(\"Tell me a joke about dogs\"):\n    print(chunk)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:18.570722Z","iopub.execute_input":"2024-09-02T18:14:18.571098Z","iopub.status.idle":"2024-09-02T18:14:20.062858Z","shell.execute_reply.started":"2024-09-02T18:14:18.571062Z","shell.execute_reply":"2024-09-02T18:14:20.061505Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'topic': 'dogs'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Prompting and parsing model outputs directly**","metadata":{}},{"cell_type":"code","source":"from typing import List\n\nfrom langchain_core.output_parsers import PydanticOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\n\nclass Person(BaseModel):\n    \"\"\"Information about a person.\"\"\"\n\n    name: str = Field(..., description=\"The name of the person\")\n    height_in_meters: float = Field(\n        ..., description=\"The height of the person expressed in meters.\"\n    )\n\n\nclass People(BaseModel):\n    \"\"\"Identifying information about all people in a text.\"\"\"\n\n    people: List[Person]\n\n\n# Set up a parser\nparser = PydanticOutputParser(pydantic_object=People)\n\n# Prompt\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n        ),\n        (\"human\", \"{query}\"),\n    ]\n).partial(format_instructions=parser.get_format_instructions())","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:20.064220Z","iopub.execute_input":"2024-09-02T18:14:20.064681Z","iopub.status.idle":"2024-09-02T18:14:20.076491Z","shell.execute_reply.started":"2024-09-02T18:14:20.064628Z","shell.execute_reply":"2024-09-02T18:14:20.075280Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"query = \"Anna is 23 years old and she is 6 feet tall\"\n\nprint(prompt.invoke(query).to_string())","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:20.079675Z","iopub.execute_input":"2024-09-02T18:14:20.080115Z","iopub.status.idle":"2024-09-02T18:14:20.102479Z","shell.execute_reply.started":"2024-09-02T18:14:20.080070Z","shell.execute_reply":"2024-09-02T18:14:20.101285Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"System: Answer the user query. Wrap the output in `json` tags\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\n```\nHuman: Anna is 23 years old and she is 6 feet tall\n","output_type":"stream"}]},{"cell_type":"code","source":"chain = prompt | llm | parser\n\nchain.invoke({\"query\": query})","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:20.104061Z","iopub.execute_input":"2024-09-02T18:14:20.104542Z","iopub.status.idle":"2024-09-02T18:14:25.696449Z","shell.execute_reply.started":"2024-09-02T18:14:20.104487Z","shell.execute_reply":"2024-09-02T18:14:25.695380Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"People(people=[Person(name='Anna', height_in_meters=1.8288)])"},"metadata":{}}]},{"cell_type":"markdown","source":"**Few shot prompt with tool calling**","metadata":{}},{"cell_type":"code","source":"from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\nexamples = [\n    HumanMessage(\n        \"What's the product of 317253 and 128472 plus four\", name=\"example_user\"\n    ),\n    AIMessage(\n        \"\",\n        name=\"example_assistant\",\n        tool_calls=[\n            {\"name\": \"Multiply\", \"args\": {\"x\": 317253, \"y\": 128472}, \"id\": \"1\"}\n        ],\n    ),\n    ToolMessage(\"16505054784\", tool_call_id=\"1\"),\n    AIMessage(\n        \"\",\n        name=\"example_assistant\",\n        tool_calls=[{\"name\": \"Add\", \"args\": {\"x\": 16505054784, \"y\": 4}, \"id\": \"2\"}],\n    ),\n    ToolMessage(\"16505054788\", tool_call_id=\"2\"),\n    AIMessage(\n        \"The product of 317253 and 128472 plus four is 16505054788\",\n        name=\"example_assistant\",\n    ),\n]\n\nsystem = \"\"\"You are bad at math but are an expert at using a calculator. \n\nUse past tool usage as an example of how to correctly use the tools.\"\"\"\nfew_shot_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{query}\"),\n    ]\n)\n\nchain = {\"query\": RunnablePassthrough()} | few_shot_prompt | llm_with_tools\nchain.invoke(\"Whats 119 times 8 minus 20\").tool_calls","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:25.697725Z","iopub.execute_input":"2024-09-02T18:14:25.698091Z","iopub.status.idle":"2024-09-02T18:14:27.728305Z","shell.execute_reply.started":"2024-09-02T18:14:25.698051Z","shell.execute_reply":"2024-09-02T18:14:27.726815Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[{'name': 'multiply',\n  'args': {'a': 119, 'b': 8},\n  'id': '4ef66e07420045339d0f97fcdd122173',\n  'type': 'tool_call'},\n {'name': 'add',\n  'args': {'a': 0, 'b': 0},\n  'id': 'bf0447c135e448abb05334efa04d30df',\n  'type': 'tool_call'}]"},"metadata":{}}]},{"cell_type":"code","source":"# Modify the chain to include steps for tool interaction and final AI output\nchain = {\"query\": RunnablePassthrough()} | few_shot_prompt | llm_with_tools\n\n# Execute the chain with the query\nresult = chain.invoke(\"Whats 119 times 8 minus 20\")\n\n# Extract tool calls and the final AI response\ntool_calls = result.tool_calls\ntool_calls","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:27.729703Z","iopub.execute_input":"2024-09-02T18:14:27.730148Z","iopub.status.idle":"2024-09-02T18:14:29.703279Z","shell.execute_reply.started":"2024-09-02T18:14:27.730109Z","shell.execute_reply":"2024-09-02T18:14:29.702004Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[{'name': 'multiply',\n  'args': {'a': 119, 'b': 8},\n  'id': 'ddc9af2aa5d04485bd3c49a02672640c',\n  'type': 'tool_call'},\n {'name': 'add',\n  'args': {'a': 0, 'b': 0},\n  'id': '17eea473e64e4254abf1b9c8ba69cdb3',\n  'type': 'tool_call'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"**The model shouldn't be trying to add anything yet, since it technically can't know the results of 119 * 8 yet.By adding a prompt with above examples we can correct this behavior:**","metadata":{}},{"cell_type":"code","source":"few_shot_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        *examples,\n        (\"human\", \"{query}\"),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:29.704952Z","iopub.execute_input":"2024-09-02T18:14:29.705455Z","iopub.status.idle":"2024-09-02T18:14:29.713903Z","shell.execute_reply.started":"2024-09-02T18:14:29.705405Z","shell.execute_reply":"2024-09-02T18:14:29.712661Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Modify the chain to include steps for tool interaction and final AI output\nchain = {\"query\": RunnablePassthrough()} | few_shot_prompt | llm_with_tools\n\n# Execute the chain with the query\nresult = chain.invoke(\"Whats 119 times 8 minus 20\")\n\n# Extract tool calls and the final AI response\ntool_calls = result.tool_calls\ntool_calls","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:29.715976Z","iopub.execute_input":"2024-09-02T18:14:29.716873Z","iopub.status.idle":"2024-09-02T18:14:31.951252Z","shell.execute_reply.started":"2024-09-02T18:14:29.716814Z","shell.execute_reply":"2024-09-02T18:14:31.949819Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[{'name': 'multiply',\n  'args': {'a': 119, 'b': 8},\n  'id': '89c39038e89d47708335d309cb27b8a5',\n  'type': 'tool_call'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Caching messages","metadata":{}},{"cell_type":"code","source":"# <!-- ruff: noqa: F821 -->\nfrom langchain_core.globals import set_llm_cache","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:31.952856Z","iopub.execute_input":"2024-09-02T18:14:31.954145Z","iopub.status.idle":"2024-09-02T18:14:31.959737Z","shell.execute_reply.started":"2024-09-02T18:14:31.954077Z","shell.execute_reply":"2024-09-02T18:14:31.958468Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom langchain_core.caches import InMemoryCache\n\nset_llm_cache(InMemoryCache())\n\n# The first time, it is not yet in cache, so it should take longer\nllm.invoke(\"Tell me a joke\")","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:31.961798Z","iopub.execute_input":"2024-09-02T18:14:31.962274Z","iopub.status.idle":"2024-09-02T18:14:32.535667Z","shell.execute_reply.started":"2024-09-02T18:14:31.962217Z","shell.execute_reply":"2024-09-02T18:14:32.534223Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"CPU times: user 8.01 ms, sys: 132 µs, total: 8.14 ms\nWall time: 560 ms\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"AIMessage(content='Why did the chicken cross the road? \\nTo get to the other side!', additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '45380f5d-874a-4cac-821d-7826b19dda4f', 'token_count': {'input_tokens': 202.0, 'output_tokens': 17.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '45380f5d-874a-4cac-821d-7826b19dda4f', 'token_count': {'input_tokens': 202.0, 'output_tokens': 17.0}}, id='run-1f56636c-1a6e-4f80-93c6-f0caa7014c87-0', usage_metadata={'input_tokens': 202, 'output_tokens': 17, 'total_tokens': 219})"},"metadata":{}}]},{"cell_type":"markdown","source":"**Now above response is cached so should take less time**","metadata":{}},{"cell_type":"code","source":"%%time\nllm.invoke(\"Tell me a joke\")","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:32.537097Z","iopub.execute_input":"2024-09-02T18:14:32.537499Z","iopub.status.idle":"2024-09-02T18:14:32.548934Z","shell.execute_reply.started":"2024-09-02T18:14:32.537460Z","shell.execute_reply":"2024-09-02T18:14:32.547516Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"CPU times: user 2.48 ms, sys: 65 µs, total: 2.55 ms\nWall time: 2.1 ms\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"AIMessage(content='Why did the chicken cross the road? \\nTo get to the other side!', additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '45380f5d-874a-4cac-821d-7826b19dda4f', 'token_count': {'input_tokens': 202.0, 'output_tokens': 17.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '45380f5d-874a-4cac-821d-7826b19dda4f', 'token_count': {'input_tokens': 202.0, 'output_tokens': 17.0}}, id='run-1f56636c-1a6e-4f80-93c6-f0caa7014c87-0', usage_metadata={'input_tokens': 202, 'output_tokens': 17, 'total_tokens': 219})"},"metadata":{}}]},{"cell_type":"markdown","source":"**Astream events**","metadata":{}},{"cell_type":"code","source":"idx = 0\n\nasync for event in llm.astream_events(\n    \"Write me a 1 verse song about goldfish on the moon\", version=\"v1\"\n):\n    idx += 1\n    if idx >= 5:  # Truncate the output\n        print(\"...Truncated\")\n        break\n    print(event)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:32.550943Z","iopub.execute_input":"2024-09-02T18:14:32.551473Z","iopub.status.idle":"2024-09-02T18:14:34.044542Z","shell.execute_reply.started":"2024-09-02T18:14:32.551415Z","shell.execute_reply":"2024-09-02T18:14:34.042910Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"{'event': 'on_chat_model_start', 'run_id': '8bcd8841-a769-4f98-9213-0ebfa998d2ea', 'name': 'ChatCohere', 'tags': [], 'metadata': {}, 'data': {'input': 'Write me a 1 verse song about goldfish on the moon'}, 'parent_ids': []}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1932496130.py:3: LangChainBetaWarning: This API is in beta and may change in the future.\n  async for event in llm.astream_events(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTooManyRequestsError\u001b[0m                      Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mastream_events(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite me a 1 verse song about goldfish on the moon\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m ):\n\u001b[1;32m      6\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m:  \u001b[38;5;66;03m# Truncate the output\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:1247\u001b[0m, in \u001b[0;36mRunnable.astream_events\u001b[0;34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1243\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly versions \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of the schema is currently supported.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1244\u001b[0m     )\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(event_stream):\n\u001b[0;32m-> 1247\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m event_stream:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m event\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/tracers/event_stream.py:778\u001b[0m, in \u001b[0;36m_astream_events_implementation_v1\u001b[0;34m(runnable, input, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m root_name \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, runnable\u001b[38;5;241m.\u001b[39mget_name())\n\u001b[1;32m    776\u001b[0m \u001b[38;5;66;03m# Ignoring mypy complaint about too many different union combinations\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# This arises because many of the argument types are unions\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m _astream_log_implementation(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     runnable,\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    781\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    782\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    783\u001b[0m     diff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    784\u001b[0m     with_streamed_output_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    786\u001b[0m ):\n\u001b[1;32m    787\u001b[0m     run_log \u001b[38;5;241m=\u001b[39m run_log \u001b[38;5;241m+\u001b[39m log\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m encountered_start_event:\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# Yield the start event for the root runnable.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py:670\u001b[0m, in \u001b[0;36m_astream_log_implementation\u001b[0;34m(runnable, input, config, stream, diff, with_streamed_output_list, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;66;03m# Wait for the runnable to finish, if not cancelled (eg. by break)\u001b[39;00m\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m task\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    672\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py:624\u001b[0m, in \u001b[0;36m_astream_log_implementation.<locals>.consume_astream\u001b[0;34m()\u001b[0m\n\u001b[1;32m    621\u001b[0m prev_final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    622\u001b[0m final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m runnable\u001b[38;5;241m.\u001b[39mastream(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    625\u001b[0m     prev_final_output \u001b[38;5;241m=\u001b[39m final_output\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:485\u001b[0m, in \u001b[0;36mBaseChatModel.astream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[1;32m    482\u001b[0m         e,\n\u001b[1;32m    483\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []),\n\u001b[1;32m    484\u001b[0m     )\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[1;32m    488\u001b[0m         LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]),\n\u001b[1;32m    489\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:463\u001b[0m, in \u001b[0;36mBaseChatModel.astream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m generation: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_astream(\n\u001b[1;32m    464\u001b[0m         messages,\n\u001b[1;32m    465\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    467\u001b[0m     ):\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_cohere/chat_models.py:518\u001b[0m, in \u001b[0;36mChatCohere._astream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    516\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 518\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mevent_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    520\u001b[0m         delta \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtext\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/cohere/base_client.py:2741\u001b[0m, in \u001b[0;36mAsyncBaseCohere.chat_stream\u001b[0;34m(self, message, model, preamble, chat_history, conversation_id, prompt_truncation, connectors, search_queries_only, documents, citation_quality, temperature, max_tokens, max_input_tokens, k, p, seed, stop_sequences, frequency_penalty, presence_penalty, raw_prompting, return_prompt, tools, tool_results, force_single_step, response_format, safety_mode, request_options)\u001b[0m\n\u001b[1;32m   2737\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnprocessableEntityError(\n\u001b[1;32m   2738\u001b[0m         typing\u001b[38;5;241m.\u001b[39mcast(UnprocessableEntityErrorBody, construct_type(type_\u001b[38;5;241m=\u001b[39mUnprocessableEntityErrorBody, object_\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mjson()))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2739\u001b[0m     )\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[0;32m-> 2741\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequestsError(\n\u001b[1;32m   2742\u001b[0m         typing\u001b[38;5;241m.\u001b[39mcast(TooManyRequestsErrorBody, construct_type(type_\u001b[38;5;241m=\u001b[39mTooManyRequestsErrorBody, object_\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mjson()))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2743\u001b[0m     )\n\u001b[1;32m   2744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m499\u001b[39m:\n\u001b[1;32m   2745\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientClosedRequestError(\n\u001b[1;32m   2746\u001b[0m         typing\u001b[38;5;241m.\u001b[39mcast(ClientClosedRequestErrorBody, construct_type(type_\u001b[38;5;241m=\u001b[39mClientClosedRequestErrorBody, object_\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mjson()))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2747\u001b[0m     )\n","\u001b[0;31mTooManyRequestsError\u001b[0m: status_code: 429, body: data=None message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\""],"ename":"TooManyRequestsError","evalue":"status_code: 429, body: data=None message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"","output_type":"error"}]},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"you're a good assistant.\"},\n    {\"role\": \"system\", \"content\": \"you always respond with a joke.\"},\n    {\"role\": \"user\", \"content\": \"i wonder why it's called langchain\"},\n    {\"role\": \"user\", \"content\": \"and who is harrison chasing anyways\"},\n]\nllm.invoke(messages)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:14:58.678213Z","iopub.execute_input":"2024-09-02T18:14:58.678654Z","iopub.status.idle":"2024-09-02T18:14:59.338865Z","shell.execute_reply.started":"2024-09-02T18:14:58.678610Z","shell.execute_reply":"2024-09-02T18:14:59.337131Z"},"trusted":true},"execution_count":21,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTooManyRequestsError\u001b[0m                      Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre a good assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou always respond with a joke.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      4\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi wonder why it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms called langchain\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand who is harrison chasing anyways\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      6\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:277\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    276\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 277\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    287\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:777\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    771\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    775\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    776\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 777\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:634\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    633\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 634\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    635\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    636\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    638\u001b[0m ]\n\u001b[1;32m    639\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:624\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 624\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         )\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:846\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 846\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_cohere/chat_models.py:598\u001b[0m, in \u001b[0;36mChatCohere._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[1;32m    595\u001b[0m request \u001b[38;5;241m=\u001b[39m get_cohere_chat_request(\n\u001b[1;32m    596\u001b[0m     messages, stop_sequences\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    597\u001b[0m )\n\u001b[0;32m--> 598\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m generation_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_generation_info(response)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_info:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/cohere/client.py:103\u001b[0m, in \u001b[0;36mexperimental_kwarg_decorator.<locals>._wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_kwarg(deprecated_kwarg, kwargs):\n\u001b[1;32m     99\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_kwarg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` parameter is an experimental feature and may change in future releases.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo suppress this warning, set `log_warning_experimental_features=False` when initializing the client.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m     )\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/cohere/client.py:35\u001b[0m, in \u001b[0;36mvalidate_args.<locals>._wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: typing\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m     34\u001b[0m     check_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/cohere/base_client.py:962\u001b[0m, in \u001b[0;36mBaseCohere.chat\u001b[0;34m(self, message, model, preamble, chat_history, conversation_id, prompt_truncation, connectors, search_queries_only, documents, citation_quality, temperature, max_tokens, max_input_tokens, k, p, seed, stop_sequences, frequency_penalty, presence_penalty, raw_prompting, return_prompt, tools, tool_results, force_single_step, response_format, safety_mode, request_options)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnprocessableEntityError(\n\u001b[1;32m    959\u001b[0m         typing\u001b[38;5;241m.\u001b[39mcast(UnprocessableEntityErrorBody, construct_type(type_\u001b[38;5;241m=\u001b[39mUnprocessableEntityErrorBody, object_\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mjson()))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[0;32m--> 962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequestsError(\n\u001b[1;32m    963\u001b[0m         typing\u001b[38;5;241m.\u001b[39mcast(TooManyRequestsErrorBody, construct_type(type_\u001b[38;5;241m=\u001b[39mTooManyRequestsErrorBody, object_\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mjson()))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     )\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m499\u001b[39m:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientClosedRequestError(\n\u001b[1;32m    967\u001b[0m         typing\u001b[38;5;241m.\u001b[39mcast(ClientClosedRequestErrorBody, construct_type(type_\u001b[38;5;241m=\u001b[39mClientClosedRequestErrorBody, object_\u001b[38;5;241m=\u001b[39m_response\u001b[38;5;241m.\u001b[39mjson()))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     )\n","\u001b[0;31mTooManyRequestsError\u001b[0m: status_code: 429, body: data=None message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\""],"ename":"TooManyRequestsError","evalue":"status_code: 429, body: data=None message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}